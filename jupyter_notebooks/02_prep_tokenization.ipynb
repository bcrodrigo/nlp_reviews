{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628bde29-3dff-4f82-8f7e-d8648c1fd286",
   "metadata": {},
   "source": [
    "# Preprocessing and Tokenization\n",
    "\n",
    "Rodrigo Becerra Carrillo\n",
    "\n",
    "https://github.com/bcrodrigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd3f00-0b9d-4192-99bc-309274bc5f7f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0b875-a9c2-4594-a832-98d0bbc33b8f",
   "metadata": {},
   "source": [
    "Notebook to perform Preprocessing and Tokenization on a reviews dataset of Amazon foods.\n",
    "\n",
    "The dataset was sourced from [here](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f0729-9935-4771-a56d-64ef55625182",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09857cb-8df0-4a18-89ef-9ccd245e1590",
   "metadata": {},
   "source": [
    "\n",
    "| Column Name            | Description                                                               | Data Type |\n",
    "| ---------------------- | ------------------------------------------------------------------------- | --------- |\n",
    "| Id                     | Row ID                                                                    | int64     |\n",
    "| ProductId              | Unique identifier for Product                                             | object    |\n",
    "| UserId                 | Unique identifier for User                                                | object    |\n",
    "| ProfileName            | Profile name of the user                                                  | object    |\n",
    "| HelpfulnessNumerator   | Number of users who found the review helpful                              | int64     |\n",
    "| HelpfulnessDenominator | Number of users who indicated wether they found the review helpful or not | int64     |\n",
    "| Score                  | Rating between 1 and 5                                                    | int64     |\n",
    "| Time                   | Timestamp for the review                                                  | int64     |\n",
    "| Summary                | Brief summary of the review                                               | object    |\n",
    "| Text                   | Full review                                                               | object    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004dd88c-1f68-4429-8dc8-f2f5c46dc5c3",
   "metadata": {},
   "source": [
    "Previously, we performed EDA and noticed there were no missing values, and that there was a class imbalance in teh `Score`. From the table above, we'll only use `Text` and `Score` as features and target variable, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74229884-7db4-4c08-958d-98359aaf0ff1",
   "metadata": {},
   "source": [
    "## Import Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1538921-b3fb-425c-a7a9-4d7b8a1328da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rodrigo/anaconda3/envs/nlp_env/lib/python311.zip',\n",
       " '/Users/rodrigo/anaconda3/envs/nlp_env/lib/python3.11',\n",
       " '/Users/rodrigo/anaconda3/envs/nlp_env/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/rodrigo/anaconda3/envs/nlp_env/lib/python3.11/site-packages']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2751776-b584-49e1-bc34-19cc269b6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644290ba-6e63-45dc-bb5b-417a1061ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e62c105-14ca-494f-8ea7-59b0b91a1715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebalance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Function to preprocess a reviews datascet in csv into a dataframe with score and text.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "csv_filename : str\n",
       "    Path to the csv file containing the data. Note the file is expected to be compressed using gzip.\n",
       "\n",
       "rebalance : bool, optional\n",
       "    Optional flag indicates to balance the number of reviews.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "tuple\n",
       "    Pandas DataFrames (df_orig, df_rebalanced), each with two columns: text and review score.\n",
       "\n",
       "    if rebalance is False\n",
       "        df_orig : contains all records\n",
       "        df_rebalanced : is an empty dataframe\n",
       "\n",
       "    if rebalance is True\n",
       "        df_orig : contains all records minus those used to rebalance the review score\n",
       "        df_rebalanced : contains all records used to balanced number of reviews by score\n",
       "\n",
       "    Note that in either case pd.concat([df_orig,df_rebalanced]) equals to all the records in the original dataset.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/Github/nlp_reviews/src/preprocessing.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b434828-8be1-4d0c-9e2a-5045bd80bf39",
   "metadata": {},
   "source": [
    "## Import Libraries and Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7d1d9f-a58f-4c77-b121-ae60dd33f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c164a57-7368-45bc-ba3c-dfb911bc02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/Reviews.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79cbc127-4ccc-42d4-96be-9c7bd4680e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dforig, dfnew = preprocess_dataset(file_path,rebalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f718d79-566f-434c-9631-faab1e5970af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440534, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dforig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fac622df-29dc-4807-8709-4e2e4795feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127920, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a95fbac-a534-4987-b208-f1f8c9b60bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape[0] + dforig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "651822c4-570f-45c1-a066-7a32b20321ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "0    42640\n",
       "1    42640\n",
       "2    42640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8c80fe9-38d3-4978-a928-0b2a6da529d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "2    401137\n",
       "0     39397\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dforig['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39df71-9194-4054-80bd-a0379e4f6e93",
   "metadata": {},
   "source": [
    "We'll now calculate for each score what is the average length of a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b7c08-b4b8-4c25-8b0d-6db6cadc94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Score','Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cd8d-ff32-4cf9-b40b-c97043093d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_n_char'] = df['Text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee0e41-443f-45c8-8957-1251d8c5144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Score','review_n_char']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea59de4-3528-40d1-a11f-2ac7dc4ecd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df[['Score','review_n_char']].groupby('Score').aggregate('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b2519-e6df-4b8b-b291-14333c30769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.plot(kind = 'bar')\n",
    "plt.title('Average number of characters')\n",
    "plt.xlabel('Review Score')\n",
    "plt.ylabel('Review Length (# of characters)')\n",
    "plt.grid()\n",
    "ticks = plt.xticks(rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8f03d-102c-4646-96bd-4aba771754da",
   "metadata": {},
   "source": [
    "From the graph above we see that, on average, there is no significant difference in the average number of characters of each review. \n",
    "\n",
    "The reviews with the highest score (5) seem to have the least number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38dd0a-d111-4e0d-990f-b04b498b9fc7",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9ed9b-d807-4504-81af-c3479a4bf69c",
   "metadata": {},
   "source": [
    "In this section we'll tokenize the contents of `dfnew`.\n",
    "\n",
    "The first approach we'll take will be through the Bag-of-Words model with Scikit-Learn.\n",
    "We need to \n",
    "\n",
    "- Instantiate an instance of CountVectorizer\n",
    "- Define a tokenizer that removes punctuation, stop words, and performs either stemming or lemmatization\n",
    "- Use spaCy to define a custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "61c4d963-f358-4ffe-93e1-9f74d05cdc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 5 lines\n",
    "dftest = dfnew['Text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "555be409-fc77-4b79-a300-94d8b38ad7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 5 reviews of the dataset\n",
    "first5_rev = dftest.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "246e79ff-e08a-4437-9fac-b145310c6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def custom_spacy_tokenizer(sentence):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    document = nlp(sentence)\n",
    "    \n",
    "    # make a list of tokens not containing stop words and punctuation\n",
    "    token_list = [token for token in document if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cad723ab-ec52-424d-9d91-ccd2a6a157fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def custom_spacy_tokenizer_lemma(sentence,nlp):\n",
    "    \n",
    "    document = nlp(sentence)\n",
    "    \n",
    "    # make a list of tokens not containing stop words and punctuation\n",
    "    # and lemmatize them\n",
    "    token_list = [token.lemma_ for token in document if not token.is_punct and not token.is_stop]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1d657d8-5617-4463-b626-6bfe932e484e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[drink,\n",
       " lot,\n",
       " sugar,\n",
       " free,\n",
       " beverages,\n",
       " TERRIBLE,\n",
       " brew,\n",
       " cup,\n",
       " smells,\n",
       " like,\n",
       " melted,\n",
       " butter,\n",
       " taste,\n",
       " good,\n",
       " waste,\n",
       " money]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer\n",
    "custom_spacy_tokenizer(first5_rev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e586787b-f9e6-44a6-a7dd-f1ea483aa8f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drink',\n",
       " 'lot',\n",
       " 'sugar',\n",
       " 'free',\n",
       " 'beverage',\n",
       " 'terrible',\n",
       " 'brew',\n",
       " 'cup',\n",
       " 'smell',\n",
       " 'like',\n",
       " 'melt',\n",
       " 'butter',\n",
       " 'taste',\n",
       " 'good',\n",
       " 'waste',\n",
       " 'money']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer with lemmatiztion\n",
    "custom_spacy_tokenizer_lemma(first5_rev[0],nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93f81c6a-a002-45cb-884d-02868385e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been giving my dog this treat for a long time,I found it here on Amazon and it is much cheaper! Now I learned that ALL chicken treats for dogs (and also cats I believe) that are MADE IN CHINA are being investigated  by the FDA because some dogs have died after consuming them. These treats and all Dogswell treats are made in China,I researched all over the web about this matter and bottom line is: why take the risk? A few sites say it is OK, most say to be cautious and others say don't buy.I threw out all the ones I bought and got new ones made in USA.I wish Amazon gave us the choice of \"made in USA\", for now I recommend everyone that has a pet to read the labels of the treats and food. Sorry this product, I don't recommend.\n"
     ]
    }
   ],
   "source": [
    "# show the full review\n",
    "print(first5_rev[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7da855fd-8502-4aab-8085-9537f32af878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[giving, dog, treat, long, time, found, Amazon, cheaper, learned, chicken, treats, dogs, cats, believe, CHINA, investigated,  , FDA, dogs, died, consuming, treats, Dogswell, treats, China, researched, web, matter, line, risk, sites, OK, cautious, buy, threw, ones, bought, got, new, ones, USA.I, wish, Amazon, gave, choice, USA, recommend, pet, read, labels, treats, food, Sorry, product, recommend]\n"
     ]
    }
   ],
   "source": [
    "# show the tokenized review\n",
    "print(custom_spacy_tokenizer(first5_rev[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98d755a9-bec6-4bdb-b788-66033274b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['give', 'dog', 'treat', 'long', 'time', 'find', 'Amazon', 'cheap', 'learn', 'chicken', 'treat', 'dog', 'cat', 'believe', 'CHINA', 'investigate', ' ', 'FDA', 'dog', 'die', 'consume', 'treat', 'dogswell', 'treat', 'China', 'research', 'web', 'matter', 'line', 'risk', 'site', 'ok', 'cautious', 'buy', 'throw', 'one', 'buy', 'get', 'new', 'one', 'USA.I', 'wish', 'Amazon', 'give', 'choice', 'USA', 'recommend', 'pet', 'read', 'label', 'treat', 'food', 'sorry', 'product', 'recommend']\n"
     ]
    }
   ],
   "source": [
    "# show the tokenized review with lemmatization\n",
    "print(custom_spacy_tokenizer_lemma(first5_rev[1],nlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7ba8da-1186-47a5-8c01-d58c35c54984",
   "metadata": {},
   "source": [
    "## Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187601c7-81a9-4dd9-96fc-543fb15b4cf3",
   "metadata": {},
   "source": [
    "- Need to take into consideration upper case words to bring them into lower case\n",
    "- What do we do about empty spaces? see `custom_spacy_tokenizer_lemma(first5_rev[1],nlp)` above. Between 'investigate' and 'FDA'\n",
    "- Test tokenizer with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71f334-65dd-43bb-be8e-337e0f8fd5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env_kernel",
   "language": "python",
   "name": "nlp_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
