{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628bde29-3dff-4f82-8f7e-d8648c1fd286",
   "metadata": {},
   "source": [
    "# Preprocessing and Tokenization\n",
    "\n",
    "Rodrigo Becerra Carrillo\n",
    "\n",
    "https://github.com/bcrodrigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd3f00-0b9d-4192-99bc-309274bc5f7f",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e0b875-a9c2-4594-a832-98d0bbc33b8f",
   "metadata": {},
   "source": [
    "Notebook to perform Preprocessing and Tokenization on a reviews dataset of Amazon foods.\n",
    "\n",
    "The dataset was sourced from [here](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f0729-9935-4771-a56d-64ef55625182",
   "metadata": {},
   "source": [
    "## Data Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09857cb-8df0-4a18-89ef-9ccd245e1590",
   "metadata": {},
   "source": [
    "\n",
    "| Column Name            | Description                                                               | Data Type |\n",
    "| ---------------------- | ------------------------------------------------------------------------- | --------- |\n",
    "| Id                     | Row ID                                                                    | int64     |\n",
    "| ProductId              | Unique identifier for Product                                             | object    |\n",
    "| UserId                 | Unique identifier for User                                                | object    |\n",
    "| ProfileName            | Profile name of the user                                                  | object    |\n",
    "| HelpfulnessNumerator   | Number of users who found the review helpful                              | int64     |\n",
    "| HelpfulnessDenominator | Number of users who indicated wether they found the review helpful or not | int64     |\n",
    "| Score                  | Rating between 1 and 5                                                    | int64     |\n",
    "| Time                   | Timestamp for the review                                                  | int64     |\n",
    "| Summary                | Brief summary of the review                                               | object    |\n",
    "| Text                   | Full review                                                               | object    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004dd88c-1f68-4429-8dc8-f2f5c46dc5c3",
   "metadata": {},
   "source": [
    "Previously, we performed EDA and noticed there were no missing values, and that there was a class imbalance in teh `Score`. From the table above, we'll only use `Text` and `Score` as features and target variable, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74229884-7db4-4c08-958d-98359aaf0ff1",
   "metadata": {},
   "source": [
    "## Import Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1538921-b3fb-425c-a7a9-4d7b8a1328da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/rodrigo/anaconda3/envs/pytorch_env/lib/python312.zip',\n",
       " '/Users/rodrigo/anaconda3/envs/pytorch_env/lib/python3.12',\n",
       " '/Users/rodrigo/anaconda3/envs/pytorch_env/lib/python3.12/lib-dynload',\n",
       " '',\n",
       " '/Users/rodrigo/anaconda3/envs/pytorch_env/lib/python3.12/site-packages',\n",
       " '/Users/rodrigo/Documents/Github/medium_articles/packages_and_modules/example_package']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2751776-b584-49e1-bc34-19cc269b6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644290ba-6e63-45dc-bb5b-417a1061ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing import preprocess_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e62c105-14ca-494f-8ea7-59b0b91a1715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mpreprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrebalance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Function to preprocess a reviews datascet in csv into a dataframe with score and text.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "csv_filename : str\n",
       "    Path to the csv file containing the data. Note the file is expected to be compressed using gzip.\n",
       "\n",
       "rebalance : bool, optional\n",
       "    Optional flag indicates to balance the number of reviews.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "tuple\n",
       "    Pandas DataFrames (df_orig, df_rebalanced), each with two columns: text and review score.\n",
       "\n",
       "    if rebalance is False\n",
       "        df_orig : contains all records\n",
       "        df_rebalanced : is an empty dataframe\n",
       "\n",
       "    if rebalance is True\n",
       "        df_orig : contains all records minus those used to rebalance the review score\n",
       "        df_rebalanced : contains all records used to balanced number of reviews by score\n",
       "\n",
       "    Note that in either case pd.concat([df_orig,df_rebalanced]) equals to all the records in the original dataset.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/Github/nlp_reviews/src/preprocessing.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocess_dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b434828-8be1-4d0c-9e2a-5045bd80bf39",
   "metadata": {},
   "source": [
    "## Import Libraries and Load DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7d1d9f-a58f-4c77-b121-ae60dd33f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c164a57-7368-45bc-ba3c-dfb911bc02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/Reviews.csv.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79cbc127-4ccc-42d4-96be-9c7bd4680e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dforig, dfnew = preprocess_dataset(file_path,rebalance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f718d79-566f-434c-9631-faab1e5970af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440534, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dforig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac622df-29dc-4807-8709-4e2e4795feb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127920, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a95fbac-a534-4987-b208-f1f8c9b60bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568454"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew.shape[0] + dforig.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651822c4-570f-45c1-a066-7a32b20321ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "0    42640\n",
       "1    42640\n",
       "2    42640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfnew['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8c80fe9-38d3-4978-a928-0b2a6da529d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "2    401137\n",
       "0     39397\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dforig['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39df71-9194-4054-80bd-a0379e4f6e93",
   "metadata": {},
   "source": [
    "We'll now calculate for each score what is the average length of a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b7c08-b4b8-4c25-8b0d-6db6cadc94dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Score','Text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b08cd8d-ff32-4cf9-b40b-c97043093d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_n_char'] = df['Text'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee0e41-443f-45c8-8957-1251d8c5144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Score','review_n_char']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea59de4-3528-40d1-a11f-2ac7dc4ecd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = df[['Score','review_n_char']].groupby('Score').aggregate('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b2519-e6df-4b8b-b291-14333c30769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df.plot(kind = 'bar')\n",
    "plt.title('Average number of characters')\n",
    "plt.xlabel('Review Score')\n",
    "plt.ylabel('Review Length (# of characters)')\n",
    "plt.grid()\n",
    "ticks = plt.xticks(rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8f03d-102c-4646-96bd-4aba771754da",
   "metadata": {},
   "source": [
    "From the graph above we see that, on average, there is no significant difference in the average number of characters of each review. \n",
    "\n",
    "The reviews with the highest score (5) seem to have the least number of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38dd0a-d111-4e0d-990f-b04b498b9fc7",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9f989-5004-4602-b1dd-79176a2e7d0f",
   "metadata": {},
   "source": [
    "In this section we'll tokenize the contents of `dfnew`.\n",
    "\n",
    "The first approach we'll take will be through the Bag-of-Words model with Scikit-Learn.\n",
    "We need to \n",
    "\n",
    "- Instantiate an instance of CountVectorizer\n",
    "- Define a tokenizer that removes punctuation, stop words, and performs either stemming or lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3227a0d0-9176-4904-b56c-5aea29c2f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9a82db-b656-4391-a4af-45121331094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52c308d-c0e8-4b5f-8910-1ca949e5044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rodrigo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import the nltk stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "ENGLISH_STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def custom_tokenizer(sentence):\n",
    "    # remove punctuation and set to lower case\n",
    "    for punctuation_mark in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation_mark,'').lower()\n",
    "\n",
    "    # split sentence into words\n",
    "    listofwords = sentence.split(' ')\n",
    "    listofstemmed_words = []\n",
    "    \n",
    "    # remove stopwords and any tokens that are just empty strings\n",
    "    for word in listofwords:\n",
    "        if (not word in ENGLISH_STOP_WORDS) and (word!=''):\n",
    "            # Stem words\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            listofstemmed_words.append(stemmed_word)\n",
    "\n",
    "    return listofstemmed_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env_kernel",
   "language": "python",
   "name": "nlp_env_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
